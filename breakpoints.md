# Breakpoints

[breakpoints.ipynb](./agent/breakpoints.ipynb)ì—ì„œëŠ” breakpointì˜ ê°œë…ê³¼ ì‚¬ìš©ì˜ˆë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ë…¸íŠ¸ë¶ì˜ ì›ë³¸ì€ [langchain-breakpoints](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/breakpoints/)ì…ë‹ˆë‹¤. 

## Simple Case

ë¨¼ì € ê°„ë‹¨í•œ ì¼€ì´ìŠ¤ì— ëŒ€í•œ breakpoint ì˜ˆì œ ì…ë‹ˆë‹¤.

```python
from typing import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

class State(TypedDict):
    input: str

def step_1(state):
    print("---Step 1---")
    pass

def step_2(state):
    print("---Step 2---")
    pass

def step_3(state):
    print("---Step 3---")
    pass

workflow = StateGraph(State)
workflow.add_node("step_1", step_1)
workflow.add_node("step_2", step_2)
workflow.add_node("step_3", step_3)
workflow.add_edge(START, "step_1")
workflow.add_edge("step_1", "step_2")
workflow.add_edge("step_2", "step_3")
workflow.add_edge("step_3", END)

# Set up memory
memory = MemorySaver()

# Add 
graph = workflow.compile(checkpointer=memory, interrupt_before=["step_3"])
```

ì´ë¥¼ í†µí•´ êµ¬í˜„ëœ workflowëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

![image](https://github.com/kyopark2014/llm-agent/assets/52392004/a52ab2ae-29a6-4a39-8b75-fb47fc166191)


ì´ì œ ì•„ë˜ì™€ ê°™ì´ ì…ë ¥ì„ ì§€ì •í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤. Step3ì—ì„œ ë©ˆì¶˜ë‹¤ìŒì— ì…ë ¥ì„ ë°›ìœ¼ë ¤ê³  ëŒ€ê¸° í•©ë‹ˆë‹¤. 

```python
initial_input = {"input": "hello world"}
thread = {"configurable": {"thread_id": "1"}}

# Run the graph until the first interruption
for event in graph.stream(initial_input, thread, stream_mode="values"):
    print(event)

user_approval = input("Do you want to go to Step 3? (yes/no): ")

if user_approval.lower() == 'yes':
    
    # If approved, continue the graph execution
    for event in graph.stream(None, thread, stream_mode="values"):
        print(event)
else:
    print("Operation cancelled by user.")
```

ì´ë•Œì˜ ì‹¤í–‰ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. Step 3ì„ ì‹¤í–‰í•˜ê¸° ì „ì— ë©ˆì¶°ì„  í›„ì— "yes"ë¥¼ ì…ë ¥í•˜ë©´, breakpoint ì´í›„ë¡œ ì‹¤í–‰ì„ ê³„ì†í•©ë‹ˆë‹¤. 

```text
{'input': 'hello world'}
---Step 1---
---Step 2---
Do you want to go to Step 3? (yes/no):  yes
---Step 3---
```

## Agent Case

ì•„ë˜ì™€ ê°™ì€ Toolì„ ì´ìš©í•˜ëŠ” ê²½ìš°ì— Breakpointsë¥¼ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![image](https://github.com/kyopark2014/llm-agent/assets/52392004/af93fc2b-b37e-4897-9377-f24a15077474)

ì•„ë˜ì™€ ê°™ì´ toolê³¼ nodeì— ëŒ€í•œ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. 

```python
@tool
def search(query: str):
    """Call to surf the web."""
    return [
        "It's sunny in San Francisco, but you better look out if you're a Gemini ğŸ˜ˆ."
    ]

tools = [search]
tool_node = ToolNode(tools)

model = chat.bind_tools(tools)

def should_continue(state):
    messages = state["messages"]
    last_message = messages[-1]
    if not last_message.tool_calls:
        return "end"
    else:
        return "continue"

def call_model(state):
    messages = state["messages"]
    response = model.invoke(messages)
    return {"messages": [response]}
```

ì´ì œ, ì•„ë˜ì™€ ê°™ì´ workflowë¥¼ ì •ì˜í•©ë‹ˆë‹¤. 

```python
workflow = StateGraph(MessagesState)

workflow.add_node("agent", call_model)
workflow.add_node("action", tool_node)
workflow.add_edge(START, "agent")
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "continue": "action",
        "end": END,
    },
)
workflow.add_edge("action", "agent")

memory = MemorySaver()

app = workflow.compile(checkpointer=memory, interrupt_before=["action"])
```

ì•„ë˜ì™€ ê°™ì´ ì‹¤í–‰í•©ë‹ˆë‹¤.

```python
from langchain_core.messages import HumanMessage

thread = {"configurable": {"thread_id": "3"}}
inputs = [HumanMessage(content="search for the weather in sf now")]
for event in app.stream({"messages": inputs}, thread, stream_mode="values"):
    event["messages"][-1].pretty_print()
```

actionì„ ì‹¤í–‰í•˜ê¸° ì „ì— ì•„ë˜ì™€ ê°™ì´ ë©ˆì¶¥ë‹ˆë‹¤. 

```text
================================ Human Message =================================

search for the weather in sf now
================================== Ai Message ==================================
Tool Calls:
  search (toolu_bdrk_01KJqMCKm1nd7ej6w3xayBSy)
 Call ID: toolu_bdrk_01KJqMCKm1nd7ej6w3xayBSy
  Args:
    query: san francisco weather
```    

ì´ì œ ì•„ë˜ì™€ ê°™ì´ Resumeì„ ìš”ì²­í•©ë‹ˆë‹¤.

```python
for event in app.stream(None, thread, stream_mode="values"):
    event["messages"][-1].pretty_print()
```

ì´í›„ ì•„ë˜ì™€ ê°™ì´ ë‚˜ë¨¸ì§€ ë™ì‘ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

```text
================================= Tool Message =================================
Name: search

["It's sunny in San Francisco, but you better look out if you're a Gemini \ud83d\ude08."]
================================== Ai Message ==================================

The search results show the current weather conditions in San Francisco. It looks like it is sunny there right now. However, the results also include a humorous astrological warning for people with the Gemini zodiac sign, which doesn't seem directly relevant to the weather query.

To summarize the key information from the search:

The current weather in San Francisco is sunny.
```


