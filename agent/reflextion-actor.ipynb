{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332516a7-eaa1-4e4a-b053-633f82b0a405",
   "metadata": {},
   "source": [
    "# Reflextion Actor\n",
    "\n",
    "https://github.com/langchain-ai/langgraph/blob/main/examples/reflexion/reflexion.ipynb?ref=blog.langchain.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65fa767f-7540-441c-b5f3-a98fafbfabcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4c43272-8bd6-44b8-bac7-a15fe8b295d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install langchain-aws langchain-community langchain --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2dc7c99-7e1c-418e-b4aa-8ff3eca11e60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from botocore.config import Config\n",
    "from langchain_aws import ChatBedrock\n",
    "bedrock_region = 'us-east-1'\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "boto3_bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=bedrock_region,\n",
    "    config=Config(\n",
    "        retries = {\n",
    "            'max_attempts': 30\n",
    "        }            \n",
    "    )\n",
    ")\n",
    "\n",
    "HUMAN_PROMPT = \"\\n\\nHuman:\"\n",
    "AI_PROMPT = \"\\n\\nAssistant:\"\n",
    "maxOutputTokens = 4096\n",
    "parameters = {\n",
    "    \"max_tokens\":maxOutputTokens,     \n",
    "    \"temperature\":0.1,\n",
    "    \"top_k\":250,\n",
    "    \"top_p\":0.9,\n",
    "    \"stop_sequences\": [HUMAN_PROMPT]\n",
    "}    \n",
    "chat = ChatBedrock(   \n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock, \n",
    "    model_kwargs=parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "244d40bd-2f01-4b38-9d1b-f14f0468dc72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "memory_chain = ConversationBufferWindowMemory(memory_key=\"chat_history\", output_key='answer', return_messages=True, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc10ac62-5af0-4123-9fac-a966b69a1a62",
   "metadata": {},
   "source": [
    "## Tavily Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb073e20-84cb-46d3-af04-39029187301d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Password: ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "api_key = getpass.getpass(\"Password:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dfade53-06b2-458f-ba52-e02d28401a43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TAVILY_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fb5b685-2b05-4caa-a309-9c48e71d2dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain-community tavily-python --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8a16960-ebbe-4cc8-ac42-65fad1117ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Password: ········\n"
     ]
    }
   ],
   "source": [
    "langsmith_api_key = getpass.getpass(\"Password:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ea58838-10a4-4129-8901-9b92aa5a74f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_API_KEY\"] = langsmith_api_key\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"reflexion agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "310162cf-6e90-4d89-8a98-925778b1bfb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요 서연이에요. 2023년 프로야구 시즌이 아직 진행 중이라 우승팀을 정확히 알려드릴 순 없지만, 현재 리그 순위와 각 팀의 전력을 바탕으로 우승 후보팀들을 말씀드릴 수 있어요. 정규시즌 잔여 경기와 가을 야구를 통해 최종 우승팀이 가려질 텐데, 그때까지 기다려 보는 게 좋겠네요. 지금까지의 성적과 전력으로 봤을 때 어떤 팀들이 유력할 것 같아요?"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'안녕하세요 서연이에요. 2023년 프로야구 시즌이 아직 진행 중이라 우승팀을 정확히 알려드릴 순 없지만, 현재 리그 순위와 각 팀의 전력을 바탕으로 우승 후보팀들을 말씀드릴 수 있어요. 정규시즌 잔여 경기와 가을 야구를 통해 최종 우승팀이 가려질 텐데, 그때까지 기다려 보는 게 좋겠네요. 지금까지의 성적과 전력으로 봤을 때 어떤 팀들이 유력할 것 같아요?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\n",
    "            \"다음의 Human과 Assistant의 친근한 이전 대화입니다.\"\n",
    "            \"Assistant은 상황에 맞는 구체적인 세부 정보를 충분히 제공합니다.\"\n",
    "            \"Assistant의 이름은 서연이고, 모르는 질문을 받으면 솔직히 모른다고 말합니다.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | chat\n",
    "\n",
    "msg = \"\"\n",
    "request = HumanMessage(\n",
    "    content=\"2023년 프로야구 우승팀은 누구인가요?\"\n",
    ")\n",
    "\n",
    "output = chain.stream({\"messages\": [request]})\n",
    "for event in output:\n",
    "    print(event.content, end=\"\")\n",
    "    msg += event.content\n",
    "    \n",
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad86a77a-f300-4ee2-a559-368ca6e95fa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Tuple, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated, List, Tuple, TypedDict, Literal, Sequence, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41f4a1f0-a51f-46f5-bc48-0a04c50db5dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "915e2a32-82f5-4912-8107-b67a18c2cf52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_chat = 0\n",
    "LLM_for_chat = [\n",
    "  {\n",
    "    \"bedrock_region\": \"us-west-2\", # Oregon\n",
    "    \"model_type\": \"claude3\",\n",
    "    \"model_id\": \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "442603fb-4cf6-474c-b77f-f4257dbecaae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_chat():\n",
    "    global selected_chat\n",
    "    \n",
    "    profile = LLM_for_chat[selected_chat]\n",
    "    bedrock_region =  profile['bedrock_region']\n",
    "    modelId = profile['model_id']\n",
    "    print(f'selected_chat: {selected_chat}, bedrock_region: {bedrock_region}, modelId: {modelId}')\n",
    "                          \n",
    "    # bedrock   \n",
    "    boto3_bedrock = boto3.client(\n",
    "        service_name='bedrock-runtime',\n",
    "        region_name=bedrock_region,\n",
    "        config=Config(\n",
    "            retries = {\n",
    "                'max_attempts': 30\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    parameters = {\n",
    "        \"max_tokens\":maxOutputTokens,     \n",
    "        \"temperature\":0.1,\n",
    "        \"top_k\":250,\n",
    "        \"top_p\":0.9,\n",
    "        \"stop_sequences\": [HUMAN_PROMPT]\n",
    "    }\n",
    "    # print('parameters: ', parameters)\n",
    "\n",
    "    chat = ChatBedrock(   # new chat model\n",
    "        model_id=modelId,\n",
    "        client=boto3_bedrock, \n",
    "        model_kwargs=parameters,\n",
    "    )    \n",
    "    \n",
    "    selected_chat = selected_chat + 1\n",
    "    if selected_chat == len(LLM_for_chat):\n",
    "        selected_chat = 0\n",
    "    \n",
    "    return chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d01879-aafe-4edb-b40b-669a3e7d92b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0baa44be-3e33-4df1-b3a4-05142430b23b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "\n",
    "search = TavilySearchAPIWrapper()\n",
    "tavily_tool = TavilySearchResults(api_wrapper=search, max_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be600c54-5644-483f-bafd-109b21aab8ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, ValidationError\n",
    "\n",
    "class Reflection(BaseModel):\n",
    "    missing: str = Field(description=\"Critique of what is missing.\")\n",
    "    superfluous: str = Field(description=\"Critique of what is superfluous\")\n",
    "\n",
    "class AnswerQuestion(BaseModel):\n",
    "    \"\"\"Answer the question. Provide an answer, reflection, and then follow up with search queries to improve the answer.\"\"\"\n",
    "\n",
    "    answer: str = Field(description=\"~250 word detailed answer to the question.\")\n",
    "    reflection: Reflection = Field(description=\"Your reflection on the initial answer.\")\n",
    "    search_queries: list[str] = Field(\n",
    "        description=\"1-3 search queries for researching improvements to address the critique of your current answer.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "114aee73-abc4-406f-8383-ebdc6e00053e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResponderWithRetries:\n",
    "    def __init__(self, runnable, validator):\n",
    "        self.runnable = runnable\n",
    "        self.validator = validator\n",
    "\n",
    "    def respond(self, state: list):\n",
    "        response = []\n",
    "        for attempt in range(3):\n",
    "            response = self.runnable.invoke(\n",
    "                {\"messages\": state}, {\"tags\": [f\"attempt:{attempt}\"]}\n",
    "            )\n",
    "            try:\n",
    "                self.validator.invoke(response)\n",
    "                return response\n",
    "            except ValidationError as e:\n",
    "                state = state + [\n",
    "                    response,\n",
    "                    ToolMessage(\n",
    "                        content=f\"{repr(e)}\\n\\nPay close attention to the function schema.\\n\\n\"\n",
    "                        + self.validator.schema_json()\n",
    "                        + \" Respond by fixing all validation errors.\",\n",
    "                        tool_call_id=response.tool_calls[0][\"id\"],\n",
    "                    ),\n",
    "                ]\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd783e05-bfb6-48a6-8acd-eec88b2b2397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "system = \"\"\"You are expert researcher.\n",
    "Current time: {time}\n",
    "\n",
    "1. {first_instruction}\n",
    "2. Reflect and critique your answer. Be severe to maximize improvement.\n",
    "3. Recommend search queries to research information and improve your answer.\"\"\"\n",
    "\n",
    "user = \"\"\"\\n\\n<system>Reflect on the user's original question and the\"\n",
    "            \" actions taken thus far. Respond using the {function_name} function.</reminder>\"\"\"\n",
    "\n",
    "actor_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"user\",user),\n",
    "    ]\n",
    ").partial(\n",
    "    time=lambda: datetime.datetime.now().isoformat(),\n",
    ")\n",
    "\n",
    "initial_answer_chain = actor_prompt_template.partial(\n",
    "    first_instruction=\"Provide a detailed ~250 word answer.\",\n",
    "    function_name=AnswerQuestion.__name__,\n",
    ") | chat.bind_tools(tools=[AnswerQuestion])\n",
    "\n",
    "validator = PydanticToolsParser(tools=[AnswerQuestion])\n",
    "\n",
    "first_responder = ResponderWithRetries(\n",
    "    runnable=initial_answer_chain, validator=validator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23eb19df-6306-49ce-a6d1-86c5f195b481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_question = \"Why is reflection useful in AI?\"\n",
    "initial = first_responder.respond([HumanMessage(content=example_question)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a4e479f-3ac9-4bef-a531-381aee35b29e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'usage': {'prompt_tokens': 1967, 'completion_tokens': 296, 'total_tokens': 2263}, 'stop_reason': 'tool_use', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 1967, 'completion_tokens': 296, 'total_tokens': 2263}, 'stop_reason': 'tool_use', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-464b21ef-653d-4957-aead-ab55f14f6336-0', tool_calls=[{'name': 'AnswerQuestion', 'args': {'answer': \"The original question asked why reflection is useful in AI systems. My initial answer covered several key reasons, including improving performance, explaining decisions, learning from mistakes, enabling self-awareness, and supporting ethical decision-making.\\n\\nTo illustrate with a concrete example, I described how a medical diagnosis AI could use reflection to analyze past cases where its diagnosis was incorrect, identify factors it missed or weighted improperly, and then adjust its diagnostic model to improve accuracy over time.\\n\\nI also mentioned the challenges of implementing effective reflection, such as the need for meta-reasoning capabilities to analyze internal processes, computational costs, and potential privacy/security concerns if the AI's self-analysis reveals sensitive information.\", 'reflection': {'missing': 'The answer could provide more technical details on specific architectures or algorithms used to enable reflection capabilities in AI systems.', 'superfluous': 'The response covers the key points concisely without extraneous information.'}, 'search_queries': ['reflection in AI architectures', 'meta-reasoning techniques for AI', 'AI self-analysis privacy and security']}, 'id': 'toolu_bdrk_018grmr7nGQVaZJs5V9mgmUS', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1967, 'output_tokens': 296, 'total_tokens': 2263})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2207f016-2819-4fdc-8b04-52e74e9bcb23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'AnswerQuestion',\n",
       "  'args': {'answer': \"Reflection is a valuable capability for AI systems as it allows them to evaluate their own performance, outputs, and decision-making processes. By reflecting, AI can identify strengths, weaknesses, and areas for improvement in order to continually refine and enhance its models, algorithms, and knowledge.\\n\\nOne key application of reflection in AI is in fields like robotics and autonomous systems, where self-monitoring and self-correction are critical. An AI agent controlling a robot or self-driving vehicle needs to constantly reflect on its actions, the environment around it, and potential consequences in order to make real-time adjustments and avoid hazards.\\n\\nReflection also supports continual learning for AI. As an AI system operates and encounters new data and situations, the ability to reflect on its current knowledge state allows it to pinpoint gaps and seek out additional information to expand its understanding.\\n\\nIn AI systems that interact with humans, such as virtual assistants or recommendation engines, reflection enables more natural and contextual communication by allowing the AI to consider the user's perspective, intentions, and situational context.\",\n",
       "   'reflection': {'missing': 'The answer could provide more specific examples or use cases demonstrating how different types of AI systems leverage reflection capabilities.',\n",
       "    'superfluous': 'The key points about why reflection is useful are covered reasonably well.'},\n",
       "   'search_queries': ['examples of reflection in deep learning models',\n",
       "    'self-monitoring autonomous vehicle AI',\n",
       "    'continual learning through reflection for virtual assistants']},\n",
       "  'id': 'toolu_bdrk_01QoU7UnEHwhP1SA96pUQp2U',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95fda3ae-55ef-42fd-a491-317076666d21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"재미있게 사는 방법은?\"\n",
    "inputs = [HumanMessage(content=query)]\n",
    "\n",
    "initial = first_responder.respond(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1bfeab7-7fd8-44a2-a883-bba055cc87a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'AnswerQuestion',\n",
       "  'args': {'answer': '세상을 재미있게 사는 방법은 개인의 가치관과 삶의 목표에 따라 다릅니다. 그러나 일반적으로 다음과 같은 방법들이 도움이 될 수 있습니다:\\n\\n1. 긍정적인 마인드를 갖는 것이 중요합니다. 작은 일상의 기쁨을 발견하고 감사하는 자세를 갖는 것이 행복감을 높일 수 있습니다. \\n\\n2. 새로운 경험과 도전을 두려워하지 말고 열린 자세로 임하는 것이 좋습니다. 새로운 취미활동이나 여행 등을 통해 삶에 활력을 불어넣을 수 있습니다.\\n\\n3. 가족, 친구 등 주변인들과의 관계를 소중히 여기고 시간을 가지는 것도 중요합니다. 인간관계는 삶의 의미와 행복감을 높여줍니다.\\n\\n4. 건강한 생활습관을 갖는 것 또한 중요합니다. 운동, 균형 잡힌 식단, 충분한 수면 등이 신체와 정신 건강에 도움이 됩니다.\\n\\n5. 자신의 재능과 열정을 발견하고 그것을 계발하는 데 시간을 투자하는 것도 의미 있는 삶을 살아가는 데 도움이 됩니다.',\n",
       "   'reflection': {'missing': '세상을 재미있게 사는 방법에 대한 구체적인 예시나 사례가 부족합니다. 또한 개인의 가치관과 삶의 목표에 따라 다를 수 있다는 점을 더 강조할 필요가 있습니다.',\n",
       "    'superfluous': '대체로 적절한 답변이지만, 일부 내용이 너무 일반적이거나 당위적인 면이 있습니다.'},\n",
       "   'search_queries': ['세상을 재미있게 사는 사람들의 사례',\n",
       "    '개인의 가치관과 삶의 목표에 따른 행복한 삶의 방식',\n",
       "    '일상 속 작은 기쁨을 발견하는 방법']},\n",
       "  'id': 'toolu_bdrk_018mAde3jBA12JKaYVTWZp46',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdda6c85-27ed-4d88-a000-007f4c732b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c285844d-1059-43f9-9e17-cf0afd1d6d24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = [AnswerQuestion]\n",
    "model = chat.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c74a0af-c441-47af-930c-0d8dd6e8f6db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182eeab0-c0f2-45b6-9448-ede024efba56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0440e49f-5aa0-4e92-acb5-c5a54caa5891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3d772-3e8d-4cb4-9f42-36f9ea133c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1985ea9-f738-436d-a1c6-4bed2c7bc52f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e601728-e9d9-4a39-ad07-84dc0b0ca1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af50ecae-44c8-4763-8cf7-60cfda8babe4",
   "metadata": {},
   "source": [
    "## Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "dc6ea135-a193-4721-9013-a394be82cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    task: str\n",
    "    messages: Annotated[list, add_messages]\n",
    "    reflection: list\n",
    "    search_queries: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "6bb4ebe2-0771-4880-b848-5d5df28cf77d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# query = \"인생을 즐겁게 사는 방법은?\"\n",
    "# query = \"베드민턴을 잘 치기 위해서 어떻게 해야하나?\"\n",
    "query = \"카레이서가 되기 위해서는 어떤 과정을 거쳐야 하나요?\"\n",
    "#query = \"Why is reflection useful in AI?\"\n",
    "#query =  \"재미있게 사는 방법은?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "558e5502-6720-4075-b0d5-6c26e9c73565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_draft(state: State):\n",
    "    message = [HumanMessage(content=state[\"task\"])]\n",
    "    \n",
    "    system = \"\"\"You are expert researcher.\n",
    "    Provide a detailed ~250 word answer..\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chat = get_chat()\n",
    "\n",
    "    chain = prompt | chat\n",
    "\n",
    "    response = chain.invoke({\n",
    "        \"messages\": message\n",
    "    })\n",
    "        \n",
    "    return {\n",
    "        \"task\": state[\"task\"],\n",
    "        \"messages\": [response]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "bf291461-daaa-48a5-af02-a9f50ca3372d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_chat: 0, bedrock_region: us-west-2, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n"
     ]
    }
   ],
   "source": [
    "output_draft = generate_draft({\"task\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "ee95ea31-3565-4b42-b9fc-8e8b3b0382fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': '카레이서가 되기 위해서는 어떤 과정을 거쳐야 하나요?',\n",
       " 'messages': [AIMessage(content='카레이서가 되기 위해서는 다음과 같은 과정을 거쳐야 합니다.\\n\\n1. 카팅 교육 이수\\n카레이서가 되기 위해서는 전문 카팅 교육을 받아야 합니다. 카팅 교육에서는 레이싱 기술, 차량 관리, 안전 수칙 등을 배웁니다. 일반적으로 1년 이상의 교육 과정을 이수해야 합니다.\\n\\n2. 아마추어 레이스 참가\\n카팅 교육을 마친 후에는 아마추어 레이스에 참가하여 실전 경험을 쌓아야 합니다. 아마추어 레이스에서 우수한 성적을 거두면 프로 레이싱팀의 주목을 받을 수 있습니다.\\n\\n3. 프로 레이싱팀 입단\\n우수한 성적을 거둔 아마추어 레이서는 프로 레이싱팀에 입단할 기회를 얻게 됩니다. 프로 레이싱팀에 입단하면 전문적인 트레이닝과 지원을 받을 수 있습니다.\\n\\n4. 프로 레이스 참가\\n프로 레이싱팀에 입단한 후에는 본격적으로 프로 레이스에 참가하게 됩니다. 프로 레이스에서 좋은 성적을 거두면 유명 레이서로 성장할 수 있습니다.\\n\\n따라서 카레이서가 되기 위해서는 전문 교육, 아마추어 레이스 경험, 프로 레이싱팀 입단, 프로 레이스 참가 등의 과정을 차근차근 밟아 나가야 합니다.', additional_kwargs={'usage': {'prompt_tokens': 57, 'completion_tokens': 566, 'total_tokens': 623}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 57, 'completion_tokens': 566, 'total_tokens': 623}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-b3c765e3-ec9a-4db1-9b37-668bf9338e5f-0', usage_metadata={'input_tokens': 57, 'output_tokens': 566, 'total_tokens': 623})]}"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_draft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc64cad9-4d7e-41b7-bfa5-6ae4da5cd54b",
   "metadata": {},
   "source": [
    "## Extract reflection and search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "e48f68d9-a679-4838-8638-1b0a371df35e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Reflection(BaseModel):\n",
    "    missing: str = Field(description=\"Critique of what is missing.\")\n",
    "    advisable: str = Field(description=\"Critique of what is helpful for better answer\")\n",
    "    superfluous: str = Field(description=\"Critique of what is superfluous\")\n",
    "\n",
    "class Research(BaseModel):\n",
    "    \"\"\"Provide reflection and then follow up with search queries to improve the answer.\"\"\"\n",
    "\n",
    "    reflection: Reflection = Field(description=\"Your reflection on the initial answer.\")\n",
    "    search_queries: list[str] = Field(\n",
    "        description=\"1-3 search queries for researching improvements to address the critique of your current answer.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b632cfdd-98f4-4ada-acfc-c20890eb661f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "8e3003b4-5205-4fd9-8223-027592079779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def research_draft(state: State):\n",
    "    print('draft: ', state[\"messages\"][-1].content)\n",
    "    \n",
    "    for attempt in range(5):\n",
    "        chat = get_chat()\n",
    "        structured_llm = chat.with_structured_output(Research, include_raw=True)\n",
    "        \n",
    "        info = structured_llm.invoke(state[\"messages\"][-1].content)\n",
    "        print('info: ', info)\n",
    "            \n",
    "        if not info['parsed'] == None:\n",
    "            parsed_info = info['parsed']\n",
    "            # print('reflection: ', parsed_info.reflection)                \n",
    "            reflection = [parsed_info.reflection.missing, parsed_info.reflection.advisable]\n",
    "            print('reflection: ', parsed_info.reflection)            \n",
    "            print('search_queries: ', parsed_info.search_queries)\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        \"task\": state[\"task\"],\n",
    "        \"messages\": state[\"messages\"],\n",
    "        \"reflection\": reflection,\n",
    "        \"search_queries\": parsed_info.search_queries\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "e6fcedb9-e3cc-4572-be5a-153ddbd0fd3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "draft:  카레이서가 되기 위해서는 다음과 같은 과정을 거쳐야 합니다.\n",
      "\n",
      "1. 카팅 교육 이수\n",
      "카레이서가 되기 위해서는 전문 카팅 교육을 받아야 합니다. 카팅 교육에서는 레이싱 기술, 차량 관리, 안전 수칙 등을 배웁니다. 일반적으로 1년 이상의 교육 과정을 이수해야 합니다.\n",
      "\n",
      "2. 아마추어 레이스 참가\n",
      "카팅 교육을 마친 후에는 아마추어 레이스에 참가하여 실전 경험을 쌓아야 합니다. 아마추어 레이스에서 우수한 성적을 거두면 프로 레이싱팀의 주목을 받을 수 있습니다.\n",
      "\n",
      "3. 프로 레이싱팀 입단\n",
      "우수한 성적을 거둔 아마추어 레이서는 프로 레이싱팀에 입단할 기회를 얻게 됩니다. 프로 레이싱팀에 입단하면 전문적인 트레이닝과 지원을 받을 수 있습니다.\n",
      "\n",
      "4. 프로 레이스 참가\n",
      "프로 레이싱팀에 입단한 후에는 본격적으로 프로 레이스에 참가하게 됩니다. 프로 레이스에서 좋은 성적을 거두면 유명 레이서로 성장할 수 있습니다.\n",
      "\n",
      "따라서 카레이서가 되기 위해서는 전문 교육, 아마추어 레이스 경험, 프로 레이싱팀 입단, 프로 레이스 참가 등의 과정을 차근차근 밟아 나가야 합니다.\n",
      "selected_chat: 0, bedrock_region: us-west-2, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "info:  {'raw': AIMessage(content='', additional_kwargs={'usage': {'prompt_tokens': 1069, 'completion_tokens': 352, 'total_tokens': 1421}, 'stop_reason': 'tool_use', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 1069, 'completion_tokens': 352, 'total_tokens': 1421}, 'stop_reason': 'tool_use', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-2352d535-a165-46d9-9108-85c4cb8d774d-0', tool_calls=[{'name': 'Research', 'args': {'reflection': {'missing': '이 답변은 카레이서가 되기 위한 과정을 잘 설명하고 있습니다. 하지만 각 단계에서 필요한 구체적인 요구사항이나 어려움에 대한 정보가 부족합니다.', 'advisable': '각 단계에서 필요한 구체적인 요구사항과 어려움을 추가하면 더 완벽한 답변이 될 것입니다. 예를 들어 카팅 교육 과정의 비용, 아마추어 레이스 참가 자격 요건, 프로 레이싱팀 입단 경쟁률 등의 정보가 도움이 될 것입니다.', 'superfluous': '이 답변에는 불필요한 내용은 없습니다.'}, 'search_queries': ['카팅 교육 비용', '아마추어 레이스 참가 자격', '프로 레이싱팀 입단 경쟁률']}, 'id': 'toolu_bdrk_01UwF6if5kBFxuvbAAzKtSX8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1069, 'output_tokens': 352, 'total_tokens': 1421}), 'parsed': Research(reflection=Reflection(missing='이 답변은 카레이서가 되기 위한 과정을 잘 설명하고 있습니다. 하지만 각 단계에서 필요한 구체적인 요구사항이나 어려움에 대한 정보가 부족합니다.', advisable='각 단계에서 필요한 구체적인 요구사항과 어려움을 추가하면 더 완벽한 답변이 될 것입니다. 예를 들어 카팅 교육 과정의 비용, 아마추어 레이스 참가 자격 요건, 프로 레이싱팀 입단 경쟁률 등의 정보가 도움이 될 것입니다.', superfluous='이 답변에는 불필요한 내용은 없습니다.'), search_queries=['카팅 교육 비용', '아마추어 레이스 참가 자격', '프로 레이싱팀 입단 경쟁률']), 'parsing_error': None}\n",
      "reflection:  missing='이 답변은 카레이서가 되기 위한 과정을 잘 설명하고 있습니다. 하지만 각 단계에서 필요한 구체적인 요구사항이나 어려움에 대한 정보가 부족합니다.' advisable='각 단계에서 필요한 구체적인 요구사항과 어려움을 추가하면 더 완벽한 답변이 될 것입니다. 예를 들어 카팅 교육 과정의 비용, 아마추어 레이스 참가 자격 요건, 프로 레이싱팀 입단 경쟁률 등의 정보가 도움이 될 것입니다.' superfluous='이 답변에는 불필요한 내용은 없습니다.'\n",
      "search_queries:  ['카팅 교육 비용', '아마추어 레이스 참가 자격', '프로 레이싱팀 입단 경쟁률']\n"
     ]
    }
   ],
   "source": [
    "output_research = research_draft(output_draft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "f0876e81-7037-4b09-aa74-7a5a1822b050",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': '카레이서가 되기 위해서는 어떤 과정을 거쳐야 하나요?',\n",
       " 'messages': [AIMessage(content='카레이서가 되기 위해서는 다음과 같은 과정을 거쳐야 합니다.\\n\\n1. 카팅 교육 이수\\n카레이서가 되기 위해서는 전문 카팅 교육을 받아야 합니다. 카팅 교육에서는 레이싱 기술, 차량 관리, 안전 수칙 등을 배웁니다. 일반적으로 1년 이상의 교육 과정을 이수해야 합니다.\\n\\n2. 아마추어 레이스 참가\\n카팅 교육을 마친 후에는 아마추어 레이스에 참가하여 실전 경험을 쌓아야 합니다. 아마추어 레이스에서 우수한 성적을 거두면 프로 레이싱팀의 주목을 받을 수 있습니다.\\n\\n3. 프로 레이싱팀 입단\\n우수한 성적을 거둔 아마추어 레이서는 프로 레이싱팀에 입단할 기회를 얻게 됩니다. 프로 레이싱팀에 입단하면 전문적인 트레이닝과 지원을 받을 수 있습니다.\\n\\n4. 프로 레이스 참가\\n프로 레이싱팀에 입단한 후에는 본격적으로 프로 레이스에 참가하게 됩니다. 프로 레이스에서 좋은 성적을 거두면 유명 레이서로 성장할 수 있습니다.\\n\\n따라서 카레이서가 되기 위해서는 전문 교육, 아마추어 레이스 경험, 프로 레이싱팀 입단, 프로 레이스 참가 등의 과정을 차근차근 밟아 나가야 합니다.', additional_kwargs={'usage': {'prompt_tokens': 57, 'completion_tokens': 566, 'total_tokens': 623}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 57, 'completion_tokens': 566, 'total_tokens': 623}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-b3c765e3-ec9a-4db1-9b37-668bf9338e5f-0', usage_metadata={'input_tokens': 57, 'output_tokens': 566, 'total_tokens': 623})],\n",
       " 'reflection': ['이 답변은 카레이서가 되기 위한 과정을 잘 설명하고 있습니다. 하지만 각 단계에서 필요한 구체적인 요구사항이나 어려움에 대한 정보가 부족합니다.',\n",
       "  '각 단계에서 필요한 구체적인 요구사항과 어려움을 추가하면 더 완벽한 답변이 될 것입니다. 예를 들어 카팅 교육 과정의 비용, 아마추어 레이스 참가 자격 요건, 프로 레이싱팀 입단 경쟁률 등의 정보가 도움이 될 것입니다.'],\n",
       " 'search_queries': ['카팅 교육 비용', '아마추어 레이스 참가 자격', '프로 레이싱팀 입단 경쟁률']}"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "baf59cea-2b80-43eb-b931-548439cb8111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve(queries: list):\n",
    "    content = []\n",
    "    \n",
    "    search = TavilySearchResults(k=2)\n",
    "    for q in queries:\n",
    "        response = search.invoke(q)     \n",
    "        for r in response:\n",
    "            content.append(r['content'])    \n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "837861b7-abf0-4e27-b410-8cebeab62413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def revise_answer(state: State):\n",
    "    system = \"\"\"\"Revise your previous answer using the new information.\n",
    "You should use the previous critique to add important information to your answer. provide the final answer with <result> tag\\\n",
    "<critique>\n",
    "{reflection}\n",
    "</critique>\n",
    "\n",
    "<information>\n",
    "{content}\n",
    "</information>\"\"\"\n",
    "    \n",
    "    content = retrieve(state['search_queries'])\n",
    "    # print('content: ', content)    \n",
    "                \n",
    "    reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "        \n",
    "    chat = get_chat()\n",
    "    reflect = reflection_prompt | chat\n",
    "        \n",
    "    messages = [HumanMessage(content=state[\"task\"])] + state[\"messages\"]\n",
    "    cls_map = {\"ai\": HumanMessage, \"human\": AIMessage}\n",
    "    translated = [messages[0]] + [\n",
    "        cls_map[msg.type](content=msg.content) for msg in messages[1:]\n",
    "    ]\n",
    "    print('translated: ', translated)\n",
    "        \n",
    "    res = reflect.invoke(\n",
    "        {\n",
    "            \"messages\": translated,\n",
    "            \"reflection\": state[\"reflection\"],\n",
    "            \"content\": content\n",
    "        }\n",
    "    )    \n",
    "                            \n",
    "    response = HumanMessage(content=res.content[res.content.find('<result>')+8:len(res.content)-9])\n",
    "    \n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "830bddac-8697-4833-99bc-13d1c235a61c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_chat: 0, bedrock_region: us-west-2, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "translated:  [HumanMessage(content='카레이서가 되기 위해서는 어떤 과정을 거쳐야 하나요?'), HumanMessage(content='카레이서가 되기 위해서는 다음과 같은 과정을 거쳐야 합니다.\\n\\n1. 카팅 교육 이수\\n카레이서가 되기 위해서는 전문 카팅 교육을 받아야 합니다. 카팅 교육에서는 레이싱 기술, 차량 관리, 안전 수칙 등을 배웁니다. 일반적으로 1년 이상의 교육 과정을 이수해야 합니다.\\n\\n2. 아마추어 레이스 참가\\n카팅 교육을 마친 후에는 아마추어 레이스에 참가하여 실전 경험을 쌓아야 합니다. 아마추어 레이스에서 우수한 성적을 거두면 프로 레이싱팀의 주목을 받을 수 있습니다.\\n\\n3. 프로 레이싱팀 입단\\n우수한 성적을 거둔 아마추어 레이서는 프로 레이싱팀에 입단할 기회를 얻게 됩니다. 프로 레이싱팀에 입단하면 전문적인 트레이닝과 지원을 받을 수 있습니다.\\n\\n4. 프로 레이스 참가\\n프로 레이싱팀에 입단한 후에는 본격적으로 프로 레이스에 참가하게 됩니다. 프로 레이스에서 좋은 성적을 거두면 유명 레이서로 성장할 수 있습니다.\\n\\n따라서 카레이서가 되기 위해서는 전문 교육, 아마추어 레이스 경험, 프로 레이싱팀 입단, 프로 레이스 참가 등의 과정을 차근차근 밟아 나가야 합니다.')]\n"
     ]
    }
   ],
   "source": [
    "output_revise = revise_answer(output_research)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "c7a5aa5d-59c2-468f-bde5-7b3f9e020d83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n카레이서가 되기 위해서는 다음과 같은 과정을 거쳐야 합니다:\\n\\n1. 카팅 교육 이수\\n- 전문 카팅 교육 기관에서 1년 이상의 교육 과정을 이수해야 함\\n- 교육비용이 상당히 높은 편이며, 일반적으로 수백만원에서 천만원 이상 소요됨\\n- 레이싱 기술, 차량 관리, 안전 수칙 등을 배움\\n\\n2. 아마추어 레이스 참가\\n- 카팅 교육을 마친 후 아마추어 레이스에 참가하여 실전 경험 쌓음\\n- 아마추어 레이스 참가 자격 요건을 충족해야 함 (예: 연령, 면허 등)\\n- 우수한 성적을 거두면 프로 레이싱팀의 주목을 받을 수 있음\\n\\n3. 프로 레이싱팀 입단\\n- 아마추어 레이스에서 우수한 성적을 거둔 레이서에게 기회가 주어짐\\n- 프로 레이싱팀 입단 경쟁이 매우 치열함\\n- 입단 시 전문적인 트레이닝과 지원을 받게 됨\\n\\n4. 프로 레이스 참가\\n- 프로 레이싱팀에 입단한 후 본격적으로 프로 레이스에 참가\\n- 프로 레이스에서 좋은 성적을 거두면 유명 레이서로 성장 가능\\n\\n따라서 전문 교육, 아마추어 레이스 경험, 프로팀 입단, 프로 레이스 참가 등 어려운 과정을 차근차근 밟아나가야 하며, 많은 노력과 인내심이 필요합니다.\\n'"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_revise[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e551fbf5-73f4-427e-b446-91da0e597e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc03b22-739f-4d92-b752-581ccc2513c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1e9f83-da21-4d74-8b52-e11be536e2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e86738-7a5b-435b-aedf-1676c4ff6a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a637c6a-5863-47a7-a934-3dcf6d77d2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a357fae3-47e9-46c4-8a3e-94cbbe2a19e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b26ded-6bb9-4cfd-a333-3b351e3e18e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759eb1a2-965d-4e00-8b51-f441c93a48e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de4a4a1-b88a-40d4-a3fb-05b575860ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31a3a11f-d038-4d96-928f-3ec575607844",
   "metadata": {},
   "source": [
    "## Revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fde26d36-2133-49b3-9b52-99124dd19516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "revise_instructions = \"\"\"Revise your previous answer using the new information.\n",
    "    - You should use the previous critique to add important information to your answer.\n",
    "        - You MUST include numerical citations in your revised answer to ensure it can be verified.\n",
    "        - Add a \"References\" section to the bottom of your answer (which does not count towards the word limit). In form of:\n",
    "            - [1] https://example.com\n",
    "            - [2] https://example.com\n",
    "    - You should use the previous critique to remove superfluous information from your answer and make SURE it is not more than 250 words.\n",
    "\"\"\"\n",
    "\n",
    "# Extend the initial answer schema to include references.\n",
    "# Forcing citation in the model encourages grounded responses\n",
    "class ReviseAnswer(AnswerQuestion):\n",
    "    \"\"\"Revise your original answer to your question. Provide an answer, reflection,\n",
    "\n",
    "    cite your reflection with references, and finally\n",
    "    add search queries to improve the answer.\"\"\"\n",
    "\n",
    "    references: list[str] = Field(\n",
    "        description=\"Citations motivating your updated answer.\"\n",
    "    )\n",
    "\n",
    "\n",
    "revision_chain = actor_prompt_template.partial(\n",
    "    first_instruction=revise_instructions,\n",
    "    function_name=ReviseAnswer.__name__,\n",
    ") | chat.bind_tools(tools=[ReviseAnswer])\n",
    "\n",
    "revision_validator = PydanticToolsParser(tools=[ReviseAnswer])\n",
    "\n",
    "revisor = ResponderWithRetries(runnable=revision_chain, validator=revision_validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ddf27dc-7fe9-4987-8dba-bc62f1b14d85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'usage': {'prompt_tokens': 1859, 'completion_tokens': 648, 'total_tokens': 2507}, 'stop_reason': 'tool_use', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 1859, 'completion_tokens': 648, 'total_tokens': 2507}, 'stop_reason': 'tool_use', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-5384485a-0b0c-4757-8cbd-becd3f1163b3-0', tool_calls=[{'name': 'ReviseAnswer', 'args': {'answer': \"Reflection is a crucial capability for AI systems as it enables them to evaluate their own performance, outputs, and decision-making processes in order to continually improve and enhance their models and knowledge. Some key applications of reflection in AI include:\\n\\n1) Self-monitoring and self-correction for robotics and autonomous systems like self-driving cars, which need to constantly reflect on their actions, environment, and potential consequences to make real-time adjustments and avoid hazards [1].\\n\\n2) Continual learning by allowing AI systems to pinpoint gaps in their knowledge as they operate and encounter new data/situations, then seek out additional information to expand their understanding [2].\\n\\n3) More natural and contextual interactions with humans for AI assistants and recommendation engines by reflecting on the user's perspective, intentions, and context [3].\\n\\n4) Enabling deep learning models to adapt and refine their neural network weights through techniques like meta-learning that involve reflecting on the model's own learning process [4].\\n\\nReferences:\\n[1] https://arxiv.org/abs/1711.07457\\n[2] https://www.microsoft.com/en-us/research/blog/towards-continual-learning-for-unbounded-machine-intelligence/  \\n[3] https://dl.acm.org/doi/10.1145/3442188.3445922\\n[4] https://lilianweng.github.io/lil-log/2018/04/08/meta-learning.html\", 'reflection': {'missing': 'The revised answer provides good coverage of different applications of reflection in AI with concrete examples and use cases.', 'superfluous': 'While the examples help illustrate the key points, the answer could potentially be more concise by removing some of the extra detail.'}, 'search_queries': ['concise examples of reflection in AI', 'summarizing key reflection capabilities for AI'], 'references': ['[1] https://arxiv.org/abs/1711.07457', '[2] https://www.microsoft.com/en-us/research/blog/towards-continual-learning-for-unbounded-machine-intelligence/', '[3] https://dl.acm.org/doi/10.1145/3442188.3445922', '[4] https://lilianweng.github.io/lil-log/2018/04/08/meta-learning.html']}, 'id': 'toolu_bdrk_01KgzxT3vJhj3dNJPMNgPW46', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1859, 'output_tokens': 648, 'total_tokens': 2507})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "revised = revisor.respond(\n",
    "    [\n",
    "        HumanMessage(content=example_question),\n",
    "        initial,\n",
    "        ToolMessage(\n",
    "            tool_call_id=initial.tool_calls[0][\"id\"],\n",
    "            content=json.dumps(\n",
    "                tavily_tool.invoke(\n",
    "                    {\"query\": initial.tool_calls[0][\"args\"][\"search_queries\"][0]}\n",
    "                )\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e1e4a7-0e45-43d0-9977-d7917beab157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f49e40a9-231f-460d-acdb-96d7c260edfc",
   "metadata": {},
   "source": [
    "### Create Tool Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f165bfae-fc2e-4638-a6b7-69abe4bb3a63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "def run_queries(search_queries: list[str], **kwargs):\n",
    "    \"\"\"Run the generated queries.\"\"\"\n",
    "    return tavily_tool.batch([{\"query\": query} for query in search_queries])\n",
    "\n",
    "\n",
    "tool_node = ToolNode(\n",
    "    [\n",
    "        StructuredTool.from_function(run_queries, name=AnswerQuestion.__name__),\n",
    "        StructuredTool.from_function(run_queries, name=ReviseAnswer.__name__),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2707a55-67d6-4920-b315-d17d5beaa70c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bdac0d0-82d3-490f-800c-0806fe9a5924",
   "metadata": {},
   "source": [
    "### Construct Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5b89587-a79d-4bb0-a28f-cb39aa5299ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "MAX_ITERATIONS = 5\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"draft\", first_responder.respond)\n",
    "builder.add_node(\"execute_tools\", tool_node)\n",
    "builder.add_node(\"revise\", revisor.respond)\n",
    "\n",
    "# draft -> execute_tools\n",
    "builder.add_edge(\"draft\", \"execute_tools\")\n",
    "# execute_tools -> revise\n",
    "builder.add_edge(\"execute_tools\", \"revise\")\n",
    "\n",
    "# Define looping logic:\n",
    "def _get_num_iterations(state: list):\n",
    "    i = 0\n",
    "    for m in state[::-1]:\n",
    "        if m.type not in {\"tool\", \"ai\"}:\n",
    "            break\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "def event_loop(state: list) -> Literal[\"execute_tools\", \"__end__\"]:\n",
    "    # in our case, we'll just stop after N plans\n",
    "    num_iterations = _get_num_iterations(state)\n",
    "    if num_iterations > MAX_ITERATIONS:\n",
    "        return END\n",
    "    return \"execute_tools\"\n",
    "\n",
    "\n",
    "# revise -> execute_tools OR end\n",
    "builder.add_conditional_edges(\"revise\", event_loop)\n",
    "builder.add_edge(START, \"draft\")\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0dd6f240-f7e2-4e06-a631-8ae00b4bbabb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f514212-cd31-4211-b550-3e93df028b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "980ad416-f273-4d40-b05b-ac0e10460d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "events = graph.stream(\n",
    "    [HumanMessage(content=\"How should we handle the climate crisis?\")],\n",
    "    stream_mode=\"values\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f45d12-8669-4a41-9421-27debde0ceca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36b17141-50e0-4060-8f91-068626562c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "InvalidUpdateError",
     "evalue": "Expected dict, got [HumanMessage(content='How should we handle the climate crisis?')]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m events \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m      2\u001b[0m     [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow should we handle the climate crisis?\u001b[39m\u001b[38;5;124m\"\u001b[39m)],\n\u001b[1;32m      3\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m )\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(events):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     step[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpretty_print()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1110\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m fut, task\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m-> 1110\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1780\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m   1778\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1779\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1780\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1783\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langgraph/pregel/retry.py:72\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     70\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langgraph/utils.py:93\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace:\n\u001b[0;32m---> 93\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     config \u001b[38;5;241m=\u001b[39m merge_configs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, config)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/runnables/base.py:1785\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1782\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1783\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1784\u001b[0m         Output,\n\u001b[0;32m-> 1785\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1788\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1793\u001b[0m     )\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1795\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/runnables/config.py:427\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    426\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langgraph/pregel/write.py:97\u001b[0m, in \u001b[0;36mChannelWrite._write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# process entries into values\u001b[39;00m\n\u001b[1;32m     94\u001b[0m values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH \u001b[38;5;28;01melse\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m entries\n\u001b[1;32m     96\u001b[0m ]\n\u001b[0;32m---> 97\u001b[0m values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     98\u001b[0m     val \u001b[38;5;28;01mif\u001b[39;00m write\u001b[38;5;241m.\u001b[39mmapper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m write\u001b[38;5;241m.\u001b[39mmapper\u001b[38;5;241m.\u001b[39minvoke(val, config)\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val, write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, entries)\n\u001b[1;32m    100\u001b[0m ]\n\u001b[1;32m    101\u001b[0m values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    102\u001b[0m     (write\u001b[38;5;241m.\u001b[39mchannel, val)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val, write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, entries)\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m write\u001b[38;5;241m.\u001b[39mskip_none \u001b[38;5;129;01mor\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    105\u001b[0m ]\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# write packets and values\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langgraph/pregel/write.py:98\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# process entries into values\u001b[39;00m\n\u001b[1;32m     94\u001b[0m values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH \u001b[38;5;28;01melse\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m entries\n\u001b[1;32m     96\u001b[0m ]\n\u001b[1;32m     97\u001b[0m values \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 98\u001b[0m     val \u001b[38;5;28;01mif\u001b[39;00m write\u001b[38;5;241m.\u001b[39mmapper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val, write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, entries)\n\u001b[1;32m    100\u001b[0m ]\n\u001b[1;32m    101\u001b[0m values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    102\u001b[0m     (write\u001b[38;5;241m.\u001b[39mchannel, val)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val, write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, entries)\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m write\u001b[38;5;241m.\u001b[39mskip_none \u001b[38;5;129;01mor\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    105\u001b[0m ]\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# write packets and values\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langgraph/utils.py:102\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[1;32m    101\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 102\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langgraph/graph/state.py:355\u001b[0m, in \u001b[0;36mCompiledStateGraph.attach_node.<locals>._get_state_key\u001b[0;34m(input, config, key)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m SKIP_WRITE\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected dict, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mInvalidUpdateError\u001b[0m: Expected dict, got [HumanMessage(content='How should we handle the climate crisis?')]"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, step in enumerate(events):\n",
    "    print(f\"Step {i}\")\n",
    "    step[-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e579d5-0a92-4f60-b70d-8ec276a723e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7f15b-0f89-4502-876f-3344e5eed760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a31cb6-9b6b-4e1f-8972-84fbee93a1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d4f55-89e0-40a3-a34e-2c151fa99695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a50249-a1d4-4240-b01d-51a25aa4483a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01e5a72-041e-431b-8760-4a35e2f5ee90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d22ca52-7c14-4779-9b62-9b2ef456a6fb",
   "metadata": {},
   "source": [
    "## State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6aa1405-3d61-41bb-8aa0-54d44c97adca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    task: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    content: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fccaf0c-c3c3-456c-99d6-14fc8c562302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generation(state: State):    \n",
    "    system = \"\"\"You are expert researcher. \n",
    "Provide a detailed ~250 word answer.\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"{question}\")\n",
    "        ]\n",
    "    )\n",
    "        \n",
    "    chat = get_chat()\n",
    "    chain = prompt | chat\n",
    "\n",
    "    response = chain.invoke(state['task'])\n",
    "    # print('response: ', response)\n",
    "        \n",
    "    return {\"draft\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3052c63c-ad35-4b6e-baf3-ae5157b8a3a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_chat: 0, bedrock_region: us-west-2, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n"
     ]
    }
   ],
   "source": [
    "output_draft = generation({\"task\":\"서울에서 제주를 거쳐서 대전으로 가는 가장 빠른 방법과 교통편을 알려주세요.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9112d2f8-f829-4203-a20c-965060da1f71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'draft': '서울에서 제주를 거쳐 대전으로 가는 가장 빠른 방법은 항공편을 이용하는 것입니다. 구체적인 경로는 다음과 같습니다:\\n\\n1. 서울에서 제주까지 항공편 이용\\n- 서울에서 제주국제공항까지 약 1시간 10분 소요\\n\\n2. 제주에서 대전까지 항공편 이용 \\n- 제주국제공항에서 대전국제공항까지 약 1시간 소요\\n\\n이렇게 하면 총 소요시간은 약 2시간 10분 정도가 됩니다. \\n\\n항공편을 이용하면 가장 빠르고 편리하게 이동할 수 있지만, 비용이 다소 높은 편입니다. 예약 상황에 따라 가격 변동이 있겠지만, 왕복 항공권 가격은 대략 20만원에서 30만원 사이일 것으로 예상됩니다.\\n\\n시간적 여유가 있다면 대중교통을 이용하는 것도 고려해볼 수 있습니다. 서울에서 제주까지는 배편을 이용하고, 제주에서 대전까지는 항공편을 이용하는 방법이 있습니다. 하지만 이 경우 이동 시간이 10시간 이상 소요되므로 시간을 많이 투자해야 합니다.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_draft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4abed-a7fc-4b8c-94f4-8788592efa32",
   "metadata": {},
   "source": [
    "## Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ede79e05-c2a9-406c-8453-740fcd17798d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1811cd0-10b5-464a-a7b7-13561680d2b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    \"\"\"List of quries as a json format\"\"\"\n",
    "\n",
    "    queries: str = Field(description=\"queries relevant to the question'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "beb056ab-6b29-4b37-bca6-5af46786650e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reflection(state: State):\n",
    "    system = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when writing the following answer. Generate a list of search queries that will gather \\\n",
    "any relevant information. Only generate 3 queries max. All queries should be words or string without numbers\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"{answer}\"),\n",
    "        ]\n",
    "    )\n",
    "        \n",
    "    chat = get_chat()\n",
    "    chain = prompt | chat\n",
    "\n",
    "    response = chain.invoke({\"answer\": state['draft']})\n",
    "    print('response: ', response.content)\n",
    "    \n",
    "    chat = get_chat()\n",
    "    structured_llm = chat.with_structured_output(Queries, include_raw=True)\n",
    "    info = structured_llm.invoke(response.content)\n",
    "    print('info: ', info)\n",
    "    \n",
    "    content = []\n",
    "    if not info['parsed'] == None:\n",
    "        queries = info['parsed']\n",
    "        print('queries: ', queries.queries)\n",
    "    \n",
    "        search = TavilySearchResults(k=2)\n",
    "        for q in json.loads(queries.queries):\n",
    "            # print('q: ', q)\n",
    "            \n",
    "            response = search.invoke(q)     \n",
    "            # print('response: ', response)        \n",
    "            for r in response:\n",
    "                content.append(r['content'])    \n",
    "    return {\n",
    "        \"content\": content,\n",
    "        \"draft\": state[\"draft\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b47a8a3c-9dc0-40fa-827f-4d0cec205ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_chat: 0, bedrock_region: us-west-2, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "response:  1. 서울 제주 항공편\n",
      "2. 제주 대전 항공편\n",
      "3. 서울 제주 대전 이동 경로\n",
      "selected_chat: 0, bedrock_region: us-west-2, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "info:  {'raw': AIMessage(content='', additional_kwargs={'usage': {'prompt_tokens': 371, 'completion_tokens': 109, 'total_tokens': 480}, 'stop_reason': 'tool_use', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 371, 'completion_tokens': 109, 'total_tokens': 480}, 'stop_reason': 'tool_use', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-1c951a56-d1e4-46fc-9c2b-5b5b161e2b11-0', tool_calls=[{'name': 'Queries', 'args': {'queries': '[\\n  \"서울에서 제주까지 가는 항공편\",\\n  \"제주에서 대전까지 가는 항공편\", \\n  \"서울에서 제주를 경유하여 대전까지 가는 이동 경로\"\\n]'}, 'id': 'toolu_bdrk_01Ea9ETTtddmZ8JFHRqsouXF', 'type': 'tool_call'}], usage_metadata={'input_tokens': 371, 'output_tokens': 109, 'total_tokens': 480}), 'parsed': Queries(queries='[\\n  \"서울에서 제주까지 가는 항공편\",\\n  \"제주에서 대전까지 가는 항공편\", \\n  \"서울에서 제주를 경유하여 대전까지 가는 이동 경로\"\\n]'), 'parsing_error': None}\n",
      "queries:  [\n",
      "  \"서울에서 제주까지 가는 항공편\",\n",
      "  \"제주에서 대전까지 가는 항공편\", \n",
      "  \"서울에서 제주를 경유하여 대전까지 가는 이동 경로\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "output_reflection = reflection(output_draft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e569f99a-00a8-4016-843e-d44bb67437fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': ['서울에서 제주으로 가는 실시간 항공편을 확인하고, 최저가 서울 제주 항공권을 트립닷컴에서 예약하세요! ... 서울에서 제주까지 운행하는 직항 항공편은 대부분 김포국제공항에서 출발하는데, 김포국제공항 출발 제주국제공항 도착 항공편 기준 평균 비행 ...',\n",
       "  \"편도 항공편 - 운항사: 티웨이 항공. 12월 3일 (화) 서울 김포에서 출발하여 제주공항에 도착하는 출국 직항 항공편 - 운항사: 티웨이 항공. ... 서울에서 제주공항까지 가는 직항 항공편만 보고 싶으세요? 검색할 때 '직항만'에 체크하세요.\",\n",
       "  '지난 1년간의 항공편 데이터를 바탕으로 서울에서 제주시(으)로 떠날 최적의 시기를 알아 보세요. 가장 저렴한 월별 및 요일별 항공편을 찾아보세요. ... 서울발 제주시행 항공편 예약 시 kayak 사용자에게 가장 인기 있는 항공사는 대한항공 및 아시아나항공입니다 ...',\n",
       "  '제주도행 일반석 항공권. 38,334원. 서울 김포 국제공항발 제주도행 항공편: 78,036원. 인천 국제 공항발 제주도행 항공편: 38,334원. 제주시 제주시 행 항공편. 제주도 도착 36,965원 출발 항공권을 찾아보세요. Fly $ [35982,city, 서울 출발 에어부산, 진에어, 티웨이항공 ...',\n",
       "  '서울(SEL)출발 제주(CJU)도착 익스피디아 항공권 특가! 편도 {flightsLowestPriceOW}~, 왕복 {flightsLowestPrice} ~. 실시간으로 저렴한 항공권 가격 비교부터 즉시 예약, 결제까지 한 번에. 전세계 항공권은 익스피디아!',\n",
       "  '대전여행 가는길 -서귀포버스터미널 800번 리무진- 경유지 : 서귀포버스터미널→ 유승한내들아파트→ 공무원연금공단→ 서귀포시청2청사→ 강창학구장→ 농업기술원→ 법화사→ 회수사거리→ 정든마을→ 롯데마트→ 신제주로타리→ 제주국제공항→ 제주터미널',\n",
       "  'Flights from Rach Gia to Kuala Lumpur 편도 expand_more 1 승객 expand_more 출발지 도착지 출발일 2024/07/23 today 도착일 today 항공편 검색 홈 항공편 한국 도착 제주 - 대구',\n",
       "  '1,000개 이상의 업체가 제공하는 제주-광주 특가 항공권을 비교하고 가장 저렴하거나 가장 빠른 비행기 표를 선택하세요. 광주 항공권 가격은 편도 ₩18,146부터 시작합니다. 날짜를 조정해 최저가 CJU-KWJ 비행기 표를 찾아보세요. 여행 날짜를 조정할 수 있다면 ...',\n",
       "  'CAD 431* 편도 expand_more 1 승객 expand_more 출발지 도착지 출발일 2024/08/14 today 도착일 today 항공편 검색 홈 항공편 서울 - 제주 제주 - 서울 부산 - 제주 서울 - 도쿄 서울 - 후쿠오카 부산 - 후쿠오카 서울 - 홍콩 서울 - 방콕 서울 - 다낭 서울 - 사이판 청주 - 제주 제주 - 부산 서울 - 푸꾸옥 서울 - 치앙마이 ...',\n",
       "  '대한민국 No.1 LCC 제주항공 홈페이지에서 국내외 다양한 노선과 편리한 예약 서비스를 제공합니다.',\n",
       "  '서울 에서 대전 거리 서울에서 대전까지의 거리는 약 160km 이며, 차로 약 2시간 정도 소요됩니다. 이 거리는 대략적인 거리이며, 차종, 교통 상황, 기상 상황 등에 따라 소요 시간이 달라질 수 있습니다. 서울에서 대전으로 가는 방법으로는 대표적으로 자가용, 기차, 버스를 이용하는 방법이 있습니다 ...',\n",
       "  '차량 없이 서울역에서 대전역로 이동하는 방법은 철도을 (를) 이용하는 것이며 소요 시간은 1시 7분, 비용은 $6 - $19입니다. 서울역에서 대전역까지 이동하는 데 얼마나 걸리나요? Seoul Station에서 Daejeon Station (으)로 이동하는 철도은 (는) 환승을 포함하여 1시 7분이 ...',\n",
       "  '서울 에서대전광역시 로 가는 기차 정보. 서울 에서 대전광역시 로 이동하는 가장 좋은 방법은 빠르고 현대적인 기차를 이용하는 것입니다. 도시 사이를 운행하는 모든 고속 열차는 선택할 수 있는 여러 클래스, 빠른 이동 시간 (약 2시간 소요), 매일 최대 111회 ...',\n",
       "  \"서울 고속버스터미널에서 출발하여 대전 복합터미널까지 가는 여정입니다. 약 2시간이 소요되며, 교통 상황에 따라 달라질 수 있습니다. 예매 방법. 1. 현장 발권. 2. '고속버스 티머니' 앱 다운로드 후, 출발지 '서울경부', 도착지 '대전복합'으로 하여 조회 후 예매 ...\",\n",
       "  '서울고속버스터미널에서 대전까지 버스, 지하철, 철도, 택시 또는 자동차로 가는 방법은 5가지가 있습니다. Rome2Rio의 여행 플래너에서 단계별 지침을 보고 티켓 요금 및 여행 시간을 비교하려면 아래의 옵션을 선택하세요. 추천 옵션. 버스 • 1시간 53분'],\n",
       " 'draft': '서울에서 제주를 거쳐 대전으로 가는 가장 빠른 방법은 항공편을 이용하는 것입니다. 구체적인 경로는 다음과 같습니다:\\n\\n1. 서울에서 제주까지 항공편 이용\\n- 서울에서 제주국제공항까지 약 1시간 10분 소요\\n\\n2. 제주에서 대전까지 항공편 이용 \\n- 제주국제공항에서 대전국제공항까지 약 1시간 소요\\n\\n이렇게 하면 총 소요시간은 약 2시간 10분 정도가 됩니다. \\n\\n항공편을 이용하면 가장 빠르고 편리하게 이동할 수 있지만, 비용이 다소 높은 편입니다. 예약 상황에 따라 가격 변동이 있겠지만, 왕복 항공권 가격은 대략 20만원에서 30만원 사이일 것으로 예상됩니다.\\n\\n시간적 여유가 있다면 대중교통을 이용하는 것도 고려해볼 수 있습니다. 서울에서 제주까지는 배편을 이용하고, 제주에서 대전까지는 항공편을 이용하는 방법이 있습니다. 하지만 이 경우 이동 시간이 10시간 이상 소요되므로 시간을 많이 투자해야 합니다.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a27c031-d1fe-4fc7-ae1c-1c88b5fa7585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13e17028-5273-4feb-9ac5-e32b1bd25a46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def revise_answer(state: State):   \n",
    "    system = \"\"\"Revise your previous answer using the new information as bellow. Then prvide the final answer with <result> tag.\n",
    "    \n",
    "    <information>\n",
    "    {content}\n",
    "    </information\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"<answer>{draft}</answer>\"),\n",
    "        ]\n",
    "    )\n",
    "            \n",
    "    chat = get_chat()\n",
    "    chain = prompt | chat\n",
    "\n",
    "    response = chain.invoke({\n",
    "        \"content\": state['content'],\n",
    "        \"draft\": state['draft']\n",
    "    })\n",
    "    print('response: ', response)\n",
    "            \n",
    "    revision_number = state[\"revision_number\"] if state.get(\"revision_number\") is not None else 1\n",
    "    return {\n",
    "        \"draft\": response, \n",
    "        \"revision_number\": revision_number + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1331f4f4-3da8-4053-be43-2837e4b89fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_chat: 0, bedrock_region: us-west-2, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "response:  content='제공된 정보를 종합하여 수정된 답변은 다음과 같습니다:\\n\\n<result>\\n서울에서 제주를 거쳐 대전으로 가는 가장 빠른 방법은 다음과 같습니다:\\n\\n1. 서울 김포공항에서 제주국제공항까지 직항 항공편 이용 (약 1시간 소요)\\n2. 제주국제공항에서 대전국제공항까지 직항 항공편 이용 (약 1시간 소요)\\n\\n총 소요시간은 약 2시간 정도입니다. 항공권 가격은 출발지, 항공사, 시기에 따라 다르지만 대략 서울-제주 편도 4만원 내외, 제주-대전 편도 4만원 내외로 왕복 총 16만원 정도가 예상됩니다.\\n\\n대중교통을 이용할 경우 서울에서 제주까지는 배편, 제주에서 대전까지는 항공편을 이용하는 방법이 있지만 이동 시간이 10시간 이상 소요되므로 시간이 많이 걸립니다.\\n</result>' additional_kwargs={'usage': {'prompt_tokens': 2624, 'completion_tokens': 371, 'total_tokens': 2995}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} response_metadata={'usage': {'prompt_tokens': 2624, 'completion_tokens': 371, 'total_tokens': 2995}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} id='run-d75927d3-60ec-49b2-b8cb-5d38d9470366-0' usage_metadata={'input_tokens': 2624, 'output_tokens': 371, 'total_tokens': 2995}\n"
     ]
    }
   ],
   "source": [
    "output_revise = revise_answer(output_reflection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c6ab9b93-2bb4-42d8-8fbe-6f23d9f63e56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'draft': AIMessage(content='제공된 정보를 종합하여 수정된 답변은 다음과 같습니다:\\n\\n<result>\\n서울에서 제주를 거쳐 대전으로 가는 가장 빠른 방법은 다음과 같습니다:\\n\\n1. 서울 김포공항에서 제주국제공항까지 직항 항공편 이용 (약 1시간 소요)\\n2. 제주국제공항에서 대전국제공항까지 직항 항공편 이용 (약 1시간 소요)\\n\\n총 소요시간은 약 2시간 정도입니다. 항공권 가격은 출발지, 항공사, 시기에 따라 다르지만 대략 서울-제주 편도 4만원 내외, 제주-대전 편도 4만원 내외로 왕복 총 16만원 정도가 예상됩니다.\\n\\n대중교통을 이용할 경우 서울에서 제주까지는 배편, 제주에서 대전까지는 항공편을 이용하는 방법이 있지만 이동 시간이 10시간 이상 소요되므로 시간이 많이 걸립니다.\\n</result>', additional_kwargs={'usage': {'prompt_tokens': 2624, 'completion_tokens': 371, 'total_tokens': 2995}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 2624, 'completion_tokens': 371, 'total_tokens': 2995}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-d75927d3-60ec-49b2-b8cb-5d38d9470366-0', usage_metadata={'input_tokens': 2624, 'output_tokens': 371, 'total_tokens': 2995}),\n",
       " 'revision_number': 2}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_revise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed5c6715-165c-440e-a61b-a71a20fb84d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: State, config):\n",
    "        max_revisions = config.get(\"configurable\", {}).get(\"max_revisions\", MAX_REVISIONS)\n",
    "        print(\"max_revisions: \", max_revisions)\n",
    "            \n",
    "        if state[\"revision_number\"] > max_revisions:\n",
    "            return \"end\"\n",
    "        return \"contine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4126817-bc44-42d6-a806-638f4335d5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "581b4267-21d4-4378-9260-f74c055bd9a4",
   "metadata": {},
   "source": [
    "## Create the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "92e5b2c9-a76d-427e-a753-0d43128962b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2885218-b803-49a3-8f08-33e53238ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"generation\", generation)\n",
    "workflow.add_node(\"reflection\", reflection)\n",
    "workflow.add_node(\"revise_answer\", revise_answer)\n",
    "\n",
    "workflow.set_entry_point(\"generation\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"revise_answer\", \n",
    "    should_continue, \n",
    "    {\n",
    "        \"end\": END, \n",
    "        \"contine\": \"reflection\"}\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"generation\", \"reflection\")\n",
    "workflow.add_edge(\"reflection\", \"revise_answer\")\n",
    "\n",
    "# graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3d2d7d25-897d-4010-8855-bb24e31e01c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f24c56e3-2c4b-410d-8991-6ad4739af4cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d178ff0e-2bc6-4ba0-a80f-374cd15d1435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ca1ea88-fa5f-48d4-bf01-c7199b4362e3",
   "metadata": {},
   "source": [
    "## RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "46a34384-897b-4588-b31a-8cad0a05fca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_chat: 0, bedrock_region: us-west-2, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Finished: generation\n",
      "selected_chat: 0, bedrock_region: us-west-2, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "response:  Here are three potential search queries to gather relevant information for the detailed response:\n",
      "\n",
      "1. \"climate change mitigation strategies\"\n",
      "2. \"renewable energy transition plans\"\n",
      "3. \"carbon pricing policies climate change\"\n",
      "selected_chat: 0, bedrock_region: us-west-2, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "info:  {'raw': AIMessage(content='', additional_kwargs={'usage': {'prompt_tokens': 371, 'completion_tokens': 52, 'total_tokens': 423}, 'stop_reason': 'tool_use', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 371, 'completion_tokens': 52, 'total_tokens': 423}, 'stop_reason': 'tool_use', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-ff06c0c3-5f2c-4bab-ad4a-3f0beebd05a8-0', tool_calls=[{'name': 'Queries', 'args': {'queries': '[\"climate change mitigation strategies\", \"renewable energy transition plans\", \"carbon pricing policies climate change\"]'}, 'id': 'toolu_bdrk_01R8XfBNZtorGUkUnnjVprZb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 371, 'output_tokens': 52, 'total_tokens': 423}), 'parsed': Queries(queries='[\"climate change mitigation strategies\", \"renewable energy transition plans\", \"carbon pricing policies climate change\"]'), 'parsing_error': None}\n",
      "queries:  [\"climate change mitigation strategies\", \"renewable energy transition plans\", \"carbon pricing policies climate change\"]\n",
      "Finished: reflection\n",
      "selected_chat: 0, bedrock_region: us-west-2, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "response:  content='Your detailed response covers many of the key strategies needed to address the climate crisis through mitigation (reducing emissions) and adaptation. Here is a summary with the <result> tag:\\n\\n<result>\\nTo tackle the climate crisis, a multi-pronged approach is needed involving:\\n\\nMitigation:\\n- Rapidly transitioning to renewable energy sources like solar, wind, and nuclear power\\n- Improving energy efficiency across all sectors\\n- Protecting and restoring carbon sinks like forests and oceans\\n- Developing and deploying carbon capture and storage technologies\\n- Putting a price on carbon emissions through taxes or cap-and-trade systems\\n\\nAdaptation:\\n- Building resilient infrastructure and disaster preparedness\\n- Developing drought-resistant agriculture and protecting biodiversity refuges\\n- Managed retreat from high-risk areas\\n\\nOther key elements:\\n- Providing financing, technology transfer, and capacity building to developing nations\\n- Unprecedented global cooperation and fundamentally transforming energy systems and economies\\n\\nThe goal is to dramatically reduce net greenhouse gas emissions to near zero by mid-century to limit the worst climate impacts.\\n</result>' additional_kwargs={'usage': {'prompt_tokens': 6375, 'completion_tokens': 239, 'total_tokens': 6614}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} response_metadata={'usage': {'prompt_tokens': 6375, 'completion_tokens': 239, 'total_tokens': 6614}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} id='run-fb76c996-8c54-4e3c-adc3-cf7f7b669175-0' usage_metadata={'input_tokens': 6375, 'output_tokens': 239, 'total_tokens': 6614}\n",
      "max_revisions:  2\n",
      "Finished: revise_answer\n",
      "selected_chat: 0, bedrock_region: us-west-2, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "response:  Here are 3 search queries to gather relevant information for the given response:\n",
      "\n",
      "1. \"climate change mitigation strategies\"\n",
      "2. \"climate change adaptation measures\"\n",
      "3. \"global cooperation climate action\"\n",
      "selected_chat: 0, bedrock_region: us-west-2, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "info:  {'raw': AIMessage(content='', additional_kwargs={'usage': {'prompt_tokens': 371, 'completion_tokens': 51, 'total_tokens': 422}, 'stop_reason': 'tool_use', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 371, 'completion_tokens': 51, 'total_tokens': 422}, 'stop_reason': 'tool_use', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-e871f10b-d097-4e71-bc62-50f47827eb72-0', tool_calls=[{'name': 'Queries', 'args': {'queries': '[\"climate change mitigation strategies\", \"climate change adaptation measures\", \"global cooperation climate action\"]'}, 'id': 'toolu_bdrk_01MGKeqVXpgTv7A2gsQMxdAo', 'type': 'tool_call'}], usage_metadata={'input_tokens': 371, 'output_tokens': 51, 'total_tokens': 422}), 'parsed': Queries(queries='[\"climate change mitigation strategies\", \"climate change adaptation measures\", \"global cooperation climate action\"]'), 'parsing_error': None}\n",
      "queries:  [\"climate change mitigation strategies\", \"climate change adaptation measures\", \"global cooperation climate action\"]\n",
      "Finished: reflection\n",
      "selected_chat: 0, bedrock_region: us-west-2, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "response:  content='The summary you provided in the <result> tag covers the key strategies for climate change mitigation and adaptation very well. It highlights the need for a multi-pronged approach involving transitioning to renewable energy, improving energy efficiency, protecting carbon sinks, developing carbon capture technologies, putting a price on emissions, building resilient infrastructure, developing drought-resistant agriculture, managed retreat from high-risk areas, providing financing and technology transfer to developing nations, and unprecedented global cooperation. The goal of dramatically reducing net greenhouse gas emissions to near zero by mid-century to limit the worst climate impacts is also clearly stated. This is a comprehensive and well-structured summary of the main strategies needed to tackle the climate crisis.' additional_kwargs={'usage': {'prompt_tokens': 4838, 'completion_tokens': 145, 'total_tokens': 4983}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} response_metadata={'usage': {'prompt_tokens': 4838, 'completion_tokens': 145, 'total_tokens': 4983}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} id='run-eb1c8e6f-ce8d-4fb9-b473-81dffc46bb17-0' usage_metadata={'input_tokens': 4838, 'output_tokens': 145, 'total_tokens': 4983}\n",
      "max_revisions:  2\n",
      "Finished: revise_answer\n",
      "Final:  content='The summary you provided in the <result> tag covers the key strategies for climate change mitigation and adaptation very well. It highlights the need for a multi-pronged approach involving transitioning to renewable energy, improving energy efficiency, protecting carbon sinks, developing carbon capture technologies, putting a price on emissions, building resilient infrastructure, developing drought-resistant agriculture, managed retreat from high-risk areas, providing financing and technology transfer to developing nations, and unprecedented global cooperation. The goal of dramatically reducing net greenhouse gas emissions to near zero by mid-century to limit the worst climate impacts is also clearly stated. This is a comprehensive and well-structured summary of the main strategies needed to tackle the climate crisis.' additional_kwargs={'usage': {'prompt_tokens': 4838, 'completion_tokens': 145, 'total_tokens': 4983}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} response_metadata={'usage': {'prompt_tokens': 4838, 'completion_tokens': 145, 'total_tokens': 4983}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} id='run-eb1c8e6f-ce8d-4fb9-b473-81dffc46bb17-0' usage_metadata={'input_tokens': 4838, 'output_tokens': 145, 'total_tokens': 4983}\n"
     ]
    }
   ],
   "source": [
    "MAX_REVISIONS = 2\n",
    "    \n",
    "inputs = {\"task\": \"How should we handle the climate crisis?\"}\n",
    "config = {\n",
    "        \"recursion_limit\": 50,\n",
    "        \"max_revisions\": 2\n",
    "}\n",
    "\n",
    "for output in app.stream(inputs, config=config):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Finished: {key}\")\n",
    "\n",
    "print(\"Final: \", value[\"draft\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d4536d38-d2e8-4a1e-9584-f4ccd1f36be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The summary you provided in the <result> tag covers the key strategies for climate change mitigation and adaptation very well. It highlights the need for a multi-pronged approach involving transitioning to renewable energy, improving energy efficiency, protecting carbon sinks, developing carbon capture technologies, putting a price on emissions, building resilient infrastructure, developing drought-resistant agriculture, managed retreat from high-risk areas, providing financing and technology transfer to developing nations, and unprecedented global cooperation. The goal of dramatically reducing net greenhouse gas emissions to near zero by mid-century to limit the worst climate impacts is also clearly stated. This is a comprehensive and well-structured summary of the main strategies needed to tackle the climate crisis.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value['draft'].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "50f3c78f-cec9-4996-b789-c394f23cfd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_chat: 0, bedrock_region: us-west-2, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Finished: generation\n",
      "selected_chat: 0, bedrock_region: us-west-2, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "response:  생성형 AI를 활용하여 공장의 생산성을 높이는 방법에 대한 정보를 수집하기 위해 다음과 같은 검색어를 사용할 수 있습니다.\n",
      "\n",
      "1. \"AI factory productivity\"\n",
      "2. \"predictive maintenance AI manufacturing\"\n",
      "3. \"AI vision quality control manufacturing\"\n",
      "selected_chat: 0, bedrock_region: us-west-2, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "info:  {'raw': AIMessage(content='', additional_kwargs={'usage': {'prompt_tokens': 423, 'completion_tokens': 51, 'total_tokens': 474}, 'stop_reason': 'tool_use', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 423, 'completion_tokens': 51, 'total_tokens': 474}, 'stop_reason': 'tool_use', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-46696ea2-a0f2-4dad-afe5-4e6172c57273-0', tool_calls=[{'name': 'Queries', 'args': {'queries': '[\"AI factory productivity\", \"predictive maintenance AI manufacturing\", \"AI vision quality control manufacturing\"]'}, 'id': 'toolu_bdrk_01X89YkXvh4H8Mog95zXZKZw', 'type': 'tool_call'}], usage_metadata={'input_tokens': 423, 'output_tokens': 51, 'total_tokens': 474}), 'parsed': Queries(queries='[\"AI factory productivity\", \"predictive maintenance AI manufacturing\", \"AI vision quality control manufacturing\"]'), 'parsing_error': None}\n",
      "queries:  [\"AI factory productivity\", \"predictive maintenance AI manufacturing\", \"AI vision quality control manufacturing\"]\n",
      "Finished: reflection\n",
      "selected_chat: 0, bedrock_region: us-west-2, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "response:  content='제공된 정보를 바탕으로 생성형 AI가 제조업에서 생산성을 높이는 데 기여할 수 있는 방법을 잘 정리하였습니다. <result>\\n\\n1. 예측 분석 및 최적화를 통한 수요 예측, 재고 관리, 공정 최적화\\n2. 예지 정비를 통한 기계 고장 예측 및 가동 중단 시간 최소화\\n3. AI 비전 시스템을 활용한 제품 결함 자동 감지 및 품질 향상\\n4. AI 기반 로봇 제어 및 공정 자동화를 통한 생산성 향상\\n5. 디지털 트윈을 활용한 가상 시뮬레이션을 통한 새로운 제품/공정 테스트\\n\\n이와 같이 생성형 AI 기술을 제조업에 적용하면 예측 분석, 예지 정비, 품질 관리, 자동화, 가상 시뮬레이션 등의 측면에서 생산성과 효율성을 크게 높일 수 있습니다.\\n</result>' additional_kwargs={'usage': {'prompt_tokens': 2859, 'completion_tokens': 360, 'total_tokens': 3219}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} response_metadata={'usage': {'prompt_tokens': 2859, 'completion_tokens': 360, 'total_tokens': 3219}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} id='run-726685e3-2d02-4107-a5ac-c1f326156bc3-0' usage_metadata={'input_tokens': 2859, 'output_tokens': 360, 'total_tokens': 3219}\n",
      "max_revisions:  2\n",
      "Finished: revise_answer\n",
      "selected_chat: 0, bedrock_region: us-west-2, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "response:  Here are three potential search queries to gather relevant information:\n",
      "\n",
      "\"generative AI applications manufacturing\"\n",
      "\"AI productivity manufacturing industry\"\n",
      "\"AI predictive maintenance quality control automation manufacturing\"\n",
      "selected_chat: 0, bedrock_region: us-west-2, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "info:  {'raw': AIMessage(content='', additional_kwargs={'usage': {'prompt_tokens': 364, 'completion_tokens': 55, 'total_tokens': 419}, 'stop_reason': 'tool_use', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 364, 'completion_tokens': 55, 'total_tokens': 419}, 'stop_reason': 'tool_use', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-e2e8128a-60a2-4b8a-b49e-345f452c967c-0', tool_calls=[{'name': 'Queries', 'args': {'queries': '[\"generative AI applications manufacturing\", \"AI productivity manufacturing industry\", \"AI predictive maintenance quality control automation manufacturing\"]'}, 'id': 'toolu_bdrk_01Wg9aKgrhLTanfKrwp6dVJb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 364, 'output_tokens': 55, 'total_tokens': 419}), 'parsed': Queries(queries='[\"generative AI applications manufacturing\", \"AI productivity manufacturing industry\", \"AI predictive maintenance quality control automation manufacturing\"]'), 'parsing_error': None}\n",
      "queries:  [\"generative AI applications manufacturing\", \"AI productivity manufacturing industry\", \"AI predictive maintenance quality control automation manufacturing\"]\n",
      "Finished: reflection\n",
      "selected_chat: 0, bedrock_region: us-west-2, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "response:  content='요약하신 내용이 매우 포괄적이고 잘 정리되어 있습니다. 생성형 AI가 제조업에서 예측 분석, 예지 정비, 품질 관리, 자동화, 가상 시뮬레이션 등 다양한 측면에서 생산성과 효율성을 높일 수 있는 방안을 잘 제시하셨습니다. 제가 추가할 내용은 없으며, 제시하신 답변이 완벽하다고 생각합니다.' additional_kwargs={'usage': {'prompt_tokens': 2781, 'completion_tokens': 171, 'total_tokens': 2952}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} response_metadata={'usage': {'prompt_tokens': 2781, 'completion_tokens': 171, 'total_tokens': 2952}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} id='run-18b870e3-d2e2-4eb5-940f-e65b970d22c6-0' usage_metadata={'input_tokens': 2781, 'output_tokens': 171, 'total_tokens': 2952}\n",
      "max_revisions:  2\n",
      "Finished: revise_answer\n",
      "Final:  content='요약하신 내용이 매우 포괄적이고 잘 정리되어 있습니다. 생성형 AI가 제조업에서 예측 분석, 예지 정비, 품질 관리, 자동화, 가상 시뮬레이션 등 다양한 측면에서 생산성과 효율성을 높일 수 있는 방안을 잘 제시하셨습니다. 제가 추가할 내용은 없으며, 제시하신 답변이 완벽하다고 생각합니다.' additional_kwargs={'usage': {'prompt_tokens': 2781, 'completion_tokens': 171, 'total_tokens': 2952}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} response_metadata={'usage': {'prompt_tokens': 2781, 'completion_tokens': 171, 'total_tokens': 2952}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'} id='run-18b870e3-d2e2-4eb5-940f-e65b970d22c6-0' usage_metadata={'input_tokens': 2781, 'output_tokens': 171, 'total_tokens': 2952}\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"task\": \"생성형 AI를 이용해 공장의 생산성을 높이는 방법은?\"}\n",
    "config = {\n",
    "        \"recursion_limit\": 50,\n",
    "        \"max_revisions\": 2\n",
    "}\n",
    "\n",
    "for output in app.stream(inputs, config=config):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Finished: {key}\")\n",
    "\n",
    "print(\"Final: \", value[\"draft\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dee324bb-770c-46cf-a146-a755f87ed548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'요약하신 내용이 매우 포괄적이고 잘 정리되어 있습니다. 생성형 AI가 제조업에서 예측 분석, 예지 정비, 품질 관리, 자동화, 가상 시뮬레이션 등 다양한 측면에서 생산성과 효율성을 높일 수 있는 방안을 잘 제시하셨습니다. 제가 추가할 내용은 없으며, 제시하신 답변이 완벽하다고 생각합니다.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value['draft'].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba271189-72b6-451b-b179-50cb0760b006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
